% CREATED BY DAVID FRISK, 2016
\chapter{Earley's Algorithm} \label{Earleys}

	The Earley parsing algorithm, named after its inventor Jay Earley, was
	first described in 1968~\cite{Earley}. It is a top-down parsing algorithm,
	meaning that it starts with analyzing the large-scale structure if its
	input and using that as guidance for what analysis is needed when diving
	deeper into more fine-scale structure and eventually single tokens.  Earley
	parsing is interesting in that it is able to perform well on several
	commonly used types of grammars, such as the LR(k) grammars, while also
	being able to parse all context-free grammars. This section will first give
	a high-level description of the algorithm, followed by a formalization of
	its mechanics, and finally describe our implementation of the algorithm in
	Agda.

	\section{Description}

		The Earley algorithm works by keeping track of a set of items
		representing possible partial parses for each position in the input
		sequence. It advances over the sets from left to right, with each step
		to the right representing the consumption, or \emph{`scanning`} of one
		token. For each set, all items in it are iterated over, and they are
		each checked if it and any item in the current or any previous set
		together imply that the parsing can continue even further, resulting in
		a new item. All new items are added to the current set, and the process
		is then repeated until no new unique items can be formed.

		The items are of the form $\earley{X}{\alpha}{\beta}{i}$, where
		$X$ is the non-terminal this item represents a partial parse of,
		$\alpha$ is the sequence of terminals and non-terminals that have
		already been successfully parsed, $\beta$ is what remains to be parsed,
		and $i$ is the index of the set where the parsing attempt of $X$ was
		started. New items can be constructed from previous items in one of
		three ways:

		\begin{itemize}
			\item

				If $\beta$ is of the form $Y\delta$, meaning that for this item
				to be completed, a $Y$ must now be parsed at this position, a
				new item $\earley{Y}{\epsilon}{\gamma}{j}$, where $j$ is the
				current position, is added to the current set.

			\item

				If $\beta$ is of the form $\epsilon$, meaning that this item
				could accept the string of tokens staring at $i$ and ending at
				the current position, $j$, we must find out why the parsing of
				$X$ was attempted in the first place. Any item $(Y \mapsto
				\alpha \cdot X\delta, k)$ in set $i$  could have predicted this
				item, and as such, we will add a new item $\earley{Y}{\alpha
				X}{\delta}{k}$ for each such item.

			\item

				If $\beta$ is of the form $a\delta$, and the next token in the
				input sequence is $a$, a new item
				$\earley{X}{\alpha a}{\delta}{i}$ is added to the next
				set.

		\end{itemize}

		The algorithm is initialized with all sets empty except for the first
		one, where a single item $\earley{S_0}{\epsilon}{S}{0}$ is
		inserted.  $S$ is the starting symbol, that is, the non-terminal which
		we want to find out if the input sequence conforms to or not. $S_0$ is
		a special marker that is only used for the initial item. When parsing
		of the input sequence is complete, if in the last set there is an item
		$\earley{S_0}{S}{\epsilon}{0}$ then the parser accepts, otherwise
		the parser rejects the input.

		Recreating the parse tree can be done via backtracking from the item
		sets. The nifty thing about this algorithm is that each item only can
		exist once in each item set, if the grammar is ambiguous and items can
		be derived is several different ways (or even an unbounded number of
		ways), the different derivations will all share their common items,
		which gives the Earley parsing algorithm the ability to correctly
		analyze ambiguous grammars.

	\section{Formalization}

		We will begin by defining a proposition for how and what items can be
		added to the different sets.

		\begin{table}[h]
			\centering
			\begin{tabular}{cc}
				\( \displaystyle \frac{}
					{\earley{S_0}{\epsilon}{S}{0} \in \mathbb{S}_0}
					~~\textrm{start } S
					\) &
				\( \displaystyle \frac
					{\earley{X}{\alpha}{Y\beta}{i} \in \mathbb{S}_j}
					{\earley{Y}{\epsilon}{\delta}{j} \in \mathbb{S}_j}
					~~Y \mapsto \delta
					\)
				\\~&~\\
				\( \displaystyle \frac
					{\earley{X}{\alpha}{a\beta}{i} \in \mathbb{S}_j}
					{\earley{X}{\alpha a}{\beta}{i} \in \mathbb{S}_{j+1}}
					\) &
				\( \displaystyle \frac
					{
						\earley{X}{\alpha}{Y\beta}{i} \in \mathbb{S}_j~~~~
						\earley{Y}{\gamma}{\epsilon}{j} \in \mathbb{S}_k
						}
					{\earley{X}{\alpha Y}{\beta}{i} \in \mathbb{S}_k}
					\)
			\end{tabular}
		\end{table}

		These inference rules match closely with the descriptions of how new
		items can be constructed, together with the initial item, and a similar
		proposition can be constructed in Agda:

		\begin{code}
			data _∙_⊢_/_⟶*_/_∙_ (G : CFG) :
			  (t u v : T *) -> N -> (N ∣ T) * -> (N ∣ T) * -> Set where

			  initial : {u : T *} {α : (N ∣ T) *} ->
			    CFG.rules G ∋ (CFG.start G , α) ->
			    G ∙ u ⊢ u / u ⟶* CFG.start G / ε ∙ α

			  scanner : {t u v : T *} {a : T} {X : N} {α β : (N ∣ T) *} ->
			    G ∙ t ⊢ u / a ∷ v ⟶* X / α ∙ r a ∷ β ->
			      G ∙ t ⊢ u / v ⟶* X / α ←∷ r a ∙ β

			  predict : {t u v : T *} {X Y : N} {α β δ : (N ∣ T) *} ->
			    CFG.rules G ∋ (Y , δ) ->
			    G ∙ t ⊢ u / v ⟶* X / α ∙ l Y ∷ β ->
			      G ∙ t ⊢ v / v ⟶* Y / ε ∙ δ

			  complet : {t u v w : T *} {X Y : N} {α β γ : (N ∣ T) *} ->
			    G ∙ t ⊢ u / v ⟶* X / α ∙ l Y ∷ β ->
			    G ∙ t ⊢ v / w ⟶* Y / γ ∙ ε ->
			      G ∙ t ⊢ u / w ⟶* X / α ←∷ l Y ∙ β
		\end{code}

		Here, a value with type $G \cdot t \vdash u / v \rightarrow^* X /
		\alpha \cdot \beta$ would correspond to an item
		$\earley{X}{\alpha}{\beta}{u}$ (but, importantly, the Agda version uses
		$u$ as a remainder of the input string rather than an index into it) in
		set $v$ that was generated while parsing the sequence $t$ with grammar
		$G$.

		There are some differences between the propositions we first wrote for
		Earley parsers and the data type given above. The addition of an
		explicit grammar and input sequence change little about the how the
		constructors can be used, but make it easier to reason about different
		grammars and ensure that the initial item will indeed only be put in
		the initial item set. The initial item has also been changed somewhat.
		Instead of introducing a special marker for the initial item, all rules
		for the starting symbol are now allowed as initial items.  This also
		has little effect on the propositions, but introduces a useful
		invariant: $\alpha \concat \beta \in \textrm{CFG.rules } G$.

		We also provide proofs that these propositions are sound and complete
		with respect to the parsing propositions provided in chapter
		\ref{Parsing}:

		\begin{code}
			sound : ∀ {G t u v w X α β} ->
			  G ∙ t ⊢ u / v ⟶* X / α ∙ β ->
			    G ⊢ v & w ∈ β ->
			    G ⊢ u & w ∈ α ++ β
			sound (initial x) b = b
			sound (scanner g) b = sound g (term b)
			sound (predict x g) b = b
			sound (complet g g₁) b =
			  let x₁ = sound g₁ empt in
			  let x₂ = in-g g₁ in
			  let x₃ = conc x₂ x₁ b in
			  sound g x₃
		\end{code}

		Soundness is fairly straightforward. The \codett{in-g} function proves
		the invariant $\alpha \concat \beta \in \textrm{CFG.rules } G$ for the
		Earley propositions. The function as shown here is not entirely
		complete: there are several instances of string concatenation that do
		not disappear during normalization, but these are not directly relevant
		to the proof at hand. Completeness follows in a similar fashion:

		\begin{code}
			complete : ∀ {t u v w X α β} {G : CFG} ->
			  G ∙ t ⊢ u / v ⟶* X / α ∙ β ->
			  G ⊢ v & w ∈ β ->
			    G ∙ t ⊢ u / w ⟶* X / α ++ β ∙ ε
			complete a empt = a
			complete a (conc x g g₁) =
			  let x₁ = predict x a in
			  let x₂ = complete x₁ g in
			  let x₃ = complet a x₂ in
			  complete x₃ g₁
			complete a (term g) = complete (scanner a) g
		\end{code}

		Like with the soundness proof some parts related to string
		concatenation equalities have been elided. Being both sound and
		complete, we expect the propositions we have given for Earley parsing
		to be reasonable enough to be useful for proving the correctness of an
		implementation of the algorithm. The close similarities between the
		constructors for the Earley propositions and the steps in the algorithm
		itself will also make it easier to show these properties for an
		implementation.

	\section{Implementation}

		We start by creating a representation for the Earley items:

		\begin{code}
			record Item (w : T *) (v : T *) : Set where
			  constructor _∘_↦_∘_
			  field
			    Y : N
			    u : T *
			    α β : (N ∣ T) *
			    .{χ} : CFG.rules G ∋ (Y , α ++ β)
			    .{ψ} : (Σ λ t -> t ++ u ≡ w)        -- u is a suffix of w
		\end{code}

		Here, $Y \cdot u \mapsto \alpha \cdot \beta : \textrm{Item } w\ v$
		represents an Earley item $\earley{Y}{\alpha}{\beta}{u}$ that was
		generated when parsing the input sequence $w$ up to, but not including
		the remainder $v$. Each item also carries with it a proof that it
		matches some rule in the grammar ($\chi$), as well as a proof that the
		index $u$ fits somewhere in the entire sequence $w$ ($\psi$). These
		proofs are declared irrelevant which means that they must be part of
		every item, but are not considered part of the data type for e.g.
		propositional equality, since we only care that the validity proofs
		exist not that equal items must have equal validity proofs (although
		they likely will).

		We continue with defining the item sets:

		\begin{code}
			data EState : T * -> T * -> Set where
			  start : {v : T *} ->
			    (rs : Item v v * ) ->
			    EState v v

			  step : {a : T} {w v : T *} ->
			    (ω : EState w (a ∷ v)) ->
			    (rs : Item w v * ) ->
			    EState w v
		\end{code}

		These are essentially lists of item sets (although each item set has a
		slightly different type due to the definition of the items), with each
		item set being a list of items. These sets will then be constructed in
		sequence in the same way that is indicated by the Earley propositions.

		In our implementation, the predict and complete steps are run together
		and only when they are completed is the scanner step run on all of the
		generated items. This is possible because the predict and complete
		steps only depend on items in the current and (for complete) previous
		sets, whereas the scanning step depends only on items in the current
		set, but generates items in the next one. This means that the items
		generated from the scanning step from the current set are irrelevant
		for the predict and complete steps in the current set.

		The scanning step is very simple:

		\begin{code}
			scanner₀ : ∀ {w v} ->
			  (a : T) ->
			  Item w (a ∷ v)* ->
			  Item w v *
			scanner₀ a ε = ε
			scanner₀ a ((X ∘ u ↦ α ∘ ε) ∷ rs) = scanner₀ a rs
			scanner₀ a ((X ∘ u ↦ α ∘ l Y ∷ β) ∷ rs) = scanner₀ a rs
			scanner₀ a ((X ∘ u ↦ α ∘ r b ∷ β [ χ ∘ ψ ]) ∷ rs) with decidₜ a b
			... | no x = scanner₀ a rs
			... | yes refl =
			  (X ∘ u ↦ α ←∷ r a ∘ β [ v-step χ ∘ ψ ]) ∷ (scanner₀ a rs)

			scanner : {w v : T *} ->
			  (a : T) ->
			  EState w (a ∷ v) ->
			  EState w v
			scanner a ω = step ω (scanner₀ a (Sₙ ω))
		\end{code}

		Here, \codett{scanner₀} filters through a set of items for items that
		can be scanned, and creates a set of the appropriate new items for
		those that can. \codett{scanner} then packages this appropriately in a
		\codett{EState}. \codett{Sₙ} returns the outermost item set from a
		\codett{EState}.

		The complete and predict steps are implemented similarly to the
		scanning step (although their results are not immediately packaged into
		an EState), and their implementations have been omitted here:

		\begin{code}
			complete : ∀ {v w} -> (i : Item w v) -> Item.β i ≡ ε ->
			  EState w v -> Item w v *

			predict : ∀ {v w Y β} -> (i : Item w v) -> Item.β i ≡ l Y ∷ β ->
			  EState w v -> Item w v *
		\end{code}

 		The complete and predict stages do, however, work together and they are
 		therefore, unlike the scanning step, from now on merged into a single
 		action:

		\begin{code}
			pred-comp₀ : ∀ {v w} ->
			  (i : Item w v) ->
			  (ω : EState w v) ->
			  Σ {Item w v *} λ as -> Unique as

			pred-comp₁ : {w n : T *} -> (ω : EState w n) ->
			  (ss : Item w n *) -> (rs : Item w n *) -> Item w n *
		\end{code}

		This does not do much more than selecting the appropriate step based on
		the item to be analyzed, and mapping this over a list of items to be
		analyzed. The result is also filtered so that all generated items are
		unique. This is then put when generating all the predicted and
		completed items for an item set:

		\begin{code}
			pred-comp₂ : {w n : T *} ->
			  (ω : EState w n) ->
			  (ss : Item w n *) ->
			  (rs : Item w n *) ->
			  (m : ℕ) ->
			  (p : suc (length (Σ.proj₁ (all-items {w} {n}) \\ ss)) ≤ m) ->
			  Unique (rs ++ ss) ->
			  EState w n
			pred-comp₂ {n} ω ss rs zero () q
			pred-comp₂ {n} ω ss ε (suc m) p q = Wₙ ω ss
			pred-comp₂ {n} ω ss rs@(r₁ ∷ _) (suc m) p q =
			  let x₁ = pred-comp₁ ω ss rs in
			  let x₂ = x₁ \\ (rs ++ ss) in
			  let p₁ = wf-pcw₃ (Σ.proj₀ all-items) p q in
			  let p₂ = wf-pcw₂ x₁ (rs ++ ss) q in
			  pred-comp₂ ω (rs ++ ss) x₂ m p₁ p₂
		\end{code}

		\codett{pred-comp₂} takes a \codett{EState}, whose last item set is not
		assumed to contain any items, a set of Items that have already been
		predicted and completed on, and as such can be considered to be
		'inert', as no new items can be generated in the current item set based
		on these that have not already been generated, and a set of items that
		have yet to be completed and predicted on. To help convince the
		termination checker that this function is total, an upper bound on the
		number of items that can still be generated, which decreases at
		iteration, is also provided.

		At each step, new items are generated based on the items in
		\codett{rs}. The now analyzed items in \codett{rs} are added to the set
		of 'inert' items (\codett{ss}), and the newly generated items, with
		duplicates between them and the already inert items filtered out, are
		considered the new set of items to be analyzed. It is also necessary to
		prove that the computation will eventually complete, which is done by
		showing that there is an upper bound (\codett{length (Σ.proj₁
		(all-items {w} {n}) \textbackslash\textbackslash ss)}) on the number of
		items not already in the 'inert' set, and that this always decreases.
		Because \codett{rs} must contain at least one element, and the maximum
		number of items in an item set is bounded, this will always hold.

		That the maximum number of items in an item set is bounded is shown by
		constructing a function that returns all possible items in a given item
		set:

		\begin{code}
			in-all : ∀ {w v} -> (i : Item w v) -> all-items ∋ i
		\end{code}

		\codett{all-items} necessarily contains many more items than will ever
		be present in any actual item set in the parser, many of which will be
		un-sound. Finding the exact set of items for the items sets is, of
		course, the problem of parsing we are already trying to solve.

		Finally, if there are no items left in \codett{rs} (the set of items to
		be generated from), the process is complete and \codett{pred-comp₂}
		returns all generated items. Given that \codett{rs} originally
		contained all items that could be created from scanning the previous
		token from the input sequence, the item set should now be complete, and
		we can package it all up in an EState.

		\begin{code}
			pred-comp : ∀ {w v} -> EState w v -> EState w v
		\end{code}

		At last, we put this together with the scanning step, and provide a
		function for parsing an input string:

		\begin{code}
			step : ∀ {w a v} ->
			  EState w (a ∷ v) ->
			  EState w v
			step {w} {a} {v} ω = scanner a (pred-comp ω)

			parse₀ : ∀ {w v} ->
			   EState w v ->
			   EState w ε
			parse₀ {v = ε} w = pred-comp w
			parse₀ {v = x ∷ v} w = parse₀ (step w)

			parse : ∀ w -> EState w ε
			parse w =
			  let x₁ = lookup (CFG.start G) (CFG.rules G) in
			  parse₀ (start (gen-initials w x₁))
		\end{code}

		This concludes our implementation of the Earley parsing algorithm.
